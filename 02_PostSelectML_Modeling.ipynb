{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostScriptML: Modeling Notebook\n",
    "### by Dolci Key "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In following this notebook, you should be able to run an Amazon SageMaker Instance. This notebook is mainly a function in which I have used to run scripts (the model code/specifications) through. \n",
    " \n",
    "Please review the python scripts in the SCRIPTS folder of the repo for further insignt into the model parameters. Code was referenced from Paul Breton's code along of AWS SageMaker on Medium.com. He suggested using the scripts which are helpful in keeping models separated. AWS also stores the models in the S3 Bucket after they have finished running. \n",
    "\n",
    "This modeling process was iterative in that, I started with the vanilla baseline script, made improvements, added additional updates, ran the mode_script_ii, and then worked each iteratively from that one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import pickle\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a AWS SageMaker Instance\n",
    "Import specific libraries in an AWS SageMaker Notebook Instance. This will not work in a normal Jupyter Notebook environment. Once you have the libraries, you will start a session and connect your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS  Sagemaker Needed using AWS Sagemaker Notebook Instance\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from tensorflow.python.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Bucket Connection\n",
    "Here I am connecting my S3 bucket. You MUST have 'sagemaker-' as the prefix on the name of your bucket for this to work. Please note that once the bucket is made, you cannot rename the bucket, however, you can move the data from one bucket to another if you make this mistake. \n",
    "\n",
    "A special thanks to Aren Carpenter for helping troubleshoot issues with the S3 buckets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-postscriptml' # AWS S3 Bucket path to dataset\n",
    "train_instance_type = 'ml.m4.xlarge' # AWS EC2 Instance used for training\n",
    "deploy_instance_type = 'ml.m4.xlarge' # AWS EC2 Instance used for deployment\n",
    "hyperparameters = {'learning_rate': 0.001, 'decay': 0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_path = 's3://{}/TRAIN'.format(bucket) # Path to training data \n",
    "validation_input_path = 's3://{}/VALIDATION'.format(bucket) # Path to validation data \n",
    "\n",
    "\n",
    "holdout_input_path = 's3://{}/TEST'.format(bucket) # Path to test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Models with Scripts \n",
    "\n",
    "Once I had my data set up, I created the model using TensorFlow. These steps varied when using Python 3 or 2.7. Using 2.7 I was able to list the training and evaluation steps, otherwise in python 3, the version of python had to be specified and training/evaluation steps had to be moved to my hyperparameter dictionary. \n",
    "\n",
    "I read in the script from my SCRIPTS folder on my repo. There you can find each Script that I tested. I then logged each accuracy and also the highest step accuracy from the evaluation to keep track of my modeling scripts. \n",
    "\n",
    "I utilized the tutorial on Amazon SageMaker and scripting from Paul Breton to base my scripts and models on for AWS. \n",
    "\n",
    "## Metrics\n",
    "\n",
    "I used binary cross-entropy as the loss function and accuracy (binary accuracy) as the metrics for this CNN. I worked to minimize loss as much as I could as the baseline had a decent accuracy, but the loss was 1.0. \n",
    "\n",
    "So far the best model is my model_script_iii which has a loss of .343 and a binary accuracy score of .9375.\n",
    "\n",
    "My next goals after MVP will be incorporating Sigmoid activation fuction to give feedback on the image in additiona to coding for other metrics such as recall that will help minimize the false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TensorFlow(\n",
    "  entry_point=os.path.join(os.path.dirname('__file__'), \"SCRIPTS/model_script_vi.py\"), # Your entry script\n",
    "  role=role,\n",
    "  framework_version='1.12.0', # TensorFlow's version\n",
    "  training_steps = 90,\n",
    "  evaluation_steps = 30, \n",
    "  hyperparameters=hyperparameters, # For python 3 you have to specify evaluation and training steps in the above hyperparameters\n",
    "  train_instance_count=1,   # \"The number of GPUs instances to use\"\n",
    "  train_instance_type=train_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model \n",
    "\n",
    "After running the code block, you will see: \n",
    "    \n",
    "Training ...\n",
    "2020-09-23 05:07:46 Starting - Starting the training job...\n",
    "2020-09-23 05:07:49 Starting - Launching requested ML instances......\n",
    "2020-09-23 05:09:11 Starting - Preparing the instances for training......\n",
    "2020-09-23 05:10:07 Downloading - Downloading input data...\n",
    "2020-09-23 05:10:47 Training - Downloading the training image...\n",
    "2020-09-23 05:11:07 Training - Training image download completed. Training in progress.\n",
    "        \n",
    "This is starting and will then start to run the model. If it errors out, it will give a traceback at the end, so make sure to keep watching the code until you see steps running. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training ...\")\n",
    "model.fit({'training': train_input_path, 'evaluation': validation_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
