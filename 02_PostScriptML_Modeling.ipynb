{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostScriptML: Modeling Notebook\n",
    "### by Dolci Key "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import pickle\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting a AWS SageMaker Instance\n",
    "Import specific libraries in an AWS SageMaker Notebook Instance. This will not work in a normal Jupyter Notebook environment. Once you have the libraries, you will start a session and connect your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sagemaker\"\n"
     ]
    }
   ],
   "source": [
    "# AWS  Sagemaker Needed using AWS Sagemaker Notebook Instance\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from tensorflow.python.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Bucket Connection\n",
    "Here I am connecting my S3 bucket. You MUST have 'sagemaker-' as the prefix on the name of your bucket for this to work. Please note that once the bucket is made, you cannot rename the bucket, however, you can move the data from one bucket to another if you make this mistake. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-postscriptml' # AWS S3 Bucket path to dataset\n",
    "train_instance_type = 'ml.m4.xlarge' # AWS EC2 Instance used for training\n",
    "deploy_instance_type = 'ml.m4.xlarge' # AWS EC2 Instance used for deployment\n",
    "hyperparameters = {'learning_rate': 0.001, 'decay': 0.0001}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_path = 's3://{}/TRAIN'.format(bucket) # Path to training data \n",
    "test_input_path = 's3://{}/TEST'.format(bucket) # Path to test data\n",
    "validation_input_path = 's3://{}/VALIDATION'.format(bucket) # Path to validation data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model \n",
    "Once I had my data set up, I created the model using TensorFlow. These steps varied when using Python 3 or 2.7. Using 2.7 I was able to list the training and evaluation steps, otherwise in python 3, the version of python had to be specified and training/evaluation steps had to be moved to my hyperparameter dictionary. \n",
    "\n",
    "I read in the script from my SCRIPTS folder on my repo. There you can find each Script that I tested. I then logged each accuracy and also the highest step accuracy from the evaluation to keep track of my modeling scripts. \n",
    "\n",
    "My baseline model\n",
    "My first run through model had an accuracy of .90625 (start_script_i.py). \n",
    "\n",
    "Accuracy is a decent measurement for my model, however, recall and F1 are what I plan on using after MVP as AWS requires specific needs and functions in order to run these on the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(\n",
    "  entry_point=os.path.join(os.path.dirname('__file__'), \"SCRIPTS/baseline_model.py\"), # Your entry script\n",
    "  role=role,\n",
    "  framework_version='1.12.0', # TensorFlow's version\n",
    "  training_steps = 100,\n",
    "  evaluation_steps = 30, \n",
    "  hyperparameters=hyperparameters, # For python 3 you have to specify evaluation and training steps in the above hyperparameters\n",
    "  train_instance_count=1,   # \"The number of GPUs instances to use\"\n",
    "  train_instance_type=train_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "2020-09-22 22:43:27 Starting - Starting the training job...\n",
      "2020-09-22 22:43:28 Starting - Launching requested ML instances......\n",
      "2020-09-22 22:44:31 Starting - Preparing the instances for training...\n",
      "2020-09-22 22:45:20 Downloading - Downloading input data...\n",
      "2020-09-22 22:45:47 Training - Downloading the training image..\u001b[34m2020-09-22 22:46:06,415 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[34m2020-09-22 22:46:06,415 INFO - root - starting train task\u001b[0m\n",
      "\u001b[34m2020-09-22 22:46:06,431 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[34mDownloading s3://sagemaker-us-east-2-997425579135/sagemaker-tensorflow-2020-09-22-22-43-26-532/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[34m2020-09-22 22:46:08,945 INFO - tf_container - ----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[34m2020-09-22 22:46:08,945 INFO - tf_container - {\"environment\": \"cloud\", \"cluster\": {\"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[34m2020-09-22 22:46:08,945 INFO - tf_container - ---------------------------------------------------------\u001b[0m\n",
      "\u001b[34m2020-09-22 22:46:08,946 INFO - tf_container - creating RunConfig:\u001b[0m\n",
      "\u001b[34m2020-09-22 22:46:08,946 INFO - tf_container - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[34m2020-09-22 22:46:08,946 INFO - tensorflow - TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'master': [u'algo-1:2222']}, u'task': {u'index': 0, u'type': u'master'}}\u001b[0m\n",
      "\u001b[34m2020-09-22 22:46:08,946 INFO - tf_container - invoking the user-provided keras_model_fn\u001b[0m\n",
      "\u001b[34m2020-09-22 22:46:09,101 INFO - tensorflow - Using the Keras model provided.\u001b[0m\n",
      "\n",
      "2020-09-22 22:46:06 Training - Training image download completed. Training in progress.\u001b[34m2020-09-22 22:47:30,514 INFO - tensorflow - Using config: {'_save_checkpoints_secs': 300, '_keep_checkpoint_max': 5, '_task_type': u'master', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f47e2130dd0>, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_session_config': device_filters: \"/job:ps\"\u001b[0m\n",
      "\u001b[34mdevice_filters: \"/job:master\"\u001b[0m\n",
      "\u001b[34mallow_soft_placement: true\u001b[0m\n",
      "\u001b[34mgraph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m, '_global_id_in_cluster': 0, '_is_chief': True, '_protocol': None, '_save_checkpoints_steps': None, '_experimental_distribute': None, '_save_summary_steps': 100, '_model_dir': u's3://sagemaker-us-east-2-997425579135/sagemaker-tensorflow-2020-09-22-22-43-26-532/checkpoints', '_master': ''}\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:30,537 INFO - tensorflow - Not using Distribute Coordinator.\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:30,538 INFO - tensorflow - Skip starting Tensorflow server as there is only one node in the cluster.\u001b[0m\n",
      "\u001b[34mFound 1957 images belonging to 2 classes.\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:31,866 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:32,392 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:32,393 INFO - tensorflow - Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from=u's3://sagemaker-us-east-2-997425579135/sagemaker-tensorflow-2020-09-22-22-43-26-532/checkpoints/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:32,393 INFO - tensorflow - Warm-starting from: (u's3://sagemaker-us-east-2-997425579135/sagemaker-tensorflow-2020-09-22-22-43-26-532/checkpoints/keras/keras_model.ckpt',)\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:32,393 INFO - tensorflow - Warm-starting variable: dense/bias; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:32,581 INFO - tensorflow - Warm-starting variable: inputs/bias; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:32,817 INFO - tensorflow - Warm-starting variable: inputs/kernel; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:33,018 INFO - tensorflow - Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:33,193 INFO - tensorflow - Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:33,359 INFO - tensorflow - Warm-starting variable: dense/kernel; prev_var_name: Unchanged\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:33,514 INFO - tensorflow - Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:35,329 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:55,144 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[34m2020-09-22 22:47:55,152 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[34m2020-09-22 22:48:04,691 INFO - tensorflow - Saving checkpoints for 0 into s3://sagemaker-us-east-2-997425579135/sagemaker-tensorflow-2020-09-22-22-43-26-532/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[34m2020-09-22 22:51:41,159 INFO - tensorflow - loss = 0.64453244, step = 1\u001b[0m\n",
      "\u001b[34m2020-09-22 22:56:34,540 INFO - tensorflow - Saving checkpoints for 92 into s3://sagemaker-us-east-2-997425579135/sagemaker-tensorflow-2020-09-22-22-43-26-532/checkpoints/model.ckpt.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Training ...\")\n",
    "estimator.fit({'training': train_input_path, 'eval': validation_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
